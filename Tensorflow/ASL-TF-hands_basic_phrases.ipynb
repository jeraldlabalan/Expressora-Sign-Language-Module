{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cef083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found 7 dataset files\n",
      "âœ… Loaded afternoon.csv (296 samples)\n",
      "âŒ Error reading combined_basic_phrases_dataset.csv: No columns to parse from file\n",
      "âœ… Loaded morning.csv (303 samples)\n",
      "âœ… Loaded night.csv (302 samples)\n",
      "âœ… Loaded today.csv (227 samples)\n",
      "âœ… Loaded tommorow.csv (295 samples)\n",
      "âœ… Loaded yesterday.csv (286 samples)\n",
      "\n",
      "âœ… Combined dataset created successfully!\n",
      "ğŸ“„ Saved to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\combined_basic_phrases_dataset.csv\n",
      "ğŸ§® Total samples: 1709\n",
      "ğŸ·ï¸ Labels: ['afternoon' 'morning' 'night' 'today' 'tommorow' 'yesterday']\n",
      "\n",
      "ğŸ”§ Cleaning and preparing data...\n",
      "ğŸ’¾ Saved label classes for later decoding\n",
      "\n",
      "ğŸ§  Building TensorFlow Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_48 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m16,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_49 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_50 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m390\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,902</span> (97.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,902\u001b[0m (97.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,902</span> (97.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,902\u001b[0m (97.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training model...\n",
      "Epoch 1/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.4648 - loss: 1.4193 - val_accuracy: 0.6971 - val_loss: 1.0487\n",
      "Epoch 2/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6780 - loss: 0.9568 - val_accuracy: 0.7336 - val_loss: 0.8392\n",
      "Epoch 3/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7466 - loss: 0.8156 - val_accuracy: 0.7737 - val_loss: 0.7457\n",
      "Epoch 4/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7685 - loss: 0.7099 - val_accuracy: 0.7701 - val_loss: 0.6741\n",
      "Epoch 5/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7914 - loss: 0.6472 - val_accuracy: 0.7920 - val_loss: 0.6260\n",
      "Epoch 6/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7887 - loss: 0.5986 - val_accuracy: 0.7847 - val_loss: 0.5740\n",
      "Epoch 7/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7969 - loss: 0.5731 - val_accuracy: 0.7920 - val_loss: 0.5342\n",
      "Epoch 8/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8042 - loss: 0.5319 - val_accuracy: 0.8577 - val_loss: 0.5149\n",
      "Epoch 9/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8353 - loss: 0.4884 - val_accuracy: 0.8467 - val_loss: 0.4658\n",
      "Epoch 10/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8472 - loss: 0.4524 - val_accuracy: 0.8504 - val_loss: 0.4381\n",
      "Epoch 11/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8664 - loss: 0.4236 - val_accuracy: 0.8613 - val_loss: 0.4098\n",
      "Epoch 12/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8719 - loss: 0.3936 - val_accuracy: 0.8759 - val_loss: 0.3693\n",
      "Epoch 13/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8737 - loss: 0.3793 - val_accuracy: 0.8942 - val_loss: 0.3422\n",
      "Epoch 14/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9012 - loss: 0.3472 - val_accuracy: 0.8869 - val_loss: 0.3175\n",
      "Epoch 15/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9021 - loss: 0.3158 - val_accuracy: 0.9526 - val_loss: 0.2672\n",
      "Epoch 16/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9195 - loss: 0.2883 - val_accuracy: 0.9489 - val_loss: 0.2414\n",
      "Epoch 17/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9140 - loss: 0.2795 - val_accuracy: 0.9708 - val_loss: 0.2235\n",
      "Epoch 18/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9177 - loss: 0.2667 - val_accuracy: 0.9672 - val_loss: 0.2117\n",
      "Epoch 19/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9341 - loss: 0.2428 - val_accuracy: 0.9708 - val_loss: 0.1948\n",
      "Epoch 20/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9405 - loss: 0.2229 - val_accuracy: 0.9708 - val_loss: 0.1882\n",
      "Epoch 21/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9378 - loss: 0.2238 - val_accuracy: 0.9599 - val_loss: 0.1692\n",
      "Epoch 22/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9387 - loss: 0.2089 - val_accuracy: 0.9635 - val_loss: 0.1704\n",
      "Epoch 23/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9497 - loss: 0.1916 - val_accuracy: 0.9672 - val_loss: 0.1556\n",
      "Epoch 24/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9488 - loss: 0.1895 - val_accuracy: 0.9489 - val_loss: 0.1768\n",
      "Epoch 25/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9460 - loss: 0.1965 - val_accuracy: 0.9599 - val_loss: 0.1566\n",
      "Epoch 26/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9442 - loss: 0.1865 - val_accuracy: 0.9526 - val_loss: 0.1368\n",
      "Epoch 27/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9442 - loss: 0.1704 - val_accuracy: 0.9708 - val_loss: 0.1387\n",
      "Epoch 28/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9607 - loss: 0.1565 - val_accuracy: 0.9781 - val_loss: 0.1171\n",
      "Epoch 29/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9597 - loss: 0.1391 - val_accuracy: 0.9854 - val_loss: 0.0992\n",
      "Epoch 30/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9533 - loss: 0.1510 - val_accuracy: 0.9635 - val_loss: 0.1216\n",
      "\n",
      "ğŸ“Š Evaluating model...\n",
      "âœ… Test Accuracy: 0.9620\n",
      "ğŸ“‰ Test Loss: 0.1131\n",
      "ğŸ’¾ Saved Keras model â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_Model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_SavedModel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='keras_tensor_170')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1438288452624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438288460304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438288454736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267497488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267499408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267499024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "ğŸ’¾ Saved TensorFlow SavedModel â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_SavedModel\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpnsxfepfc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpnsxfepfc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\JAMJAY~1\\AppData\\Local\\Temp\\tmpnsxfepfc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='keras_tensor_170')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1438288452624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438288460304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438288454736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267497488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267499408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1438267499024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "ğŸ’¾ Saved TFLite model â†’ C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\\TimeModels\\ASL_Time_Model.tflite\n",
      "\n",
      "âœ… All models exported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ASL Basic Phrases â†’ TensorFlow Deep Learning Version\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# === 1ï¸âƒ£ Dataset Loading & Combining ===\n",
    "DATA_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\Time\"\n",
    "all_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "\n",
    "print(f\"ğŸ“ Found {len(all_files)} dataset files\")\n",
    "\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if df.empty:\n",
    "            print(f\"âš ï¸ Skipped empty file: {os.path.basename(file)}\")\n",
    "            continue\n",
    "        df_list.append(df)\n",
    "        print(f\"âœ… Loaded {os.path.basename(file)} ({df.shape[0]} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {os.path.basename(file)}: {e}\")\n",
    "\n",
    "# Combine all\n",
    "if not df_list:\n",
    "    raise ValueError(\"ğŸš« No valid datasets found to merge.\")\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "combined_path = os.path.join(DATA_DIR, \"combined_basic_phrases_dataset.csv\")\n",
    "final_df.to_csv(combined_path, index=False)\n",
    "\n",
    "print(\"\\nâœ… Combined dataset created successfully!\")\n",
    "print(f\"ğŸ“„ Saved to: {combined_path}\")\n",
    "print(\"ğŸ§® Total samples:\", final_df.shape[0])\n",
    "print(\"ğŸ·ï¸ Labels:\", final_df['label'].unique())\n",
    "\n",
    "# === 2ï¸âƒ£ Preprocessing ===\n",
    "print(\"\\nğŸ”§ Cleaning and preparing data...\")\n",
    "\n",
    "# Drop NaNs and ensure numeric\n",
    "final_df = final_df.dropna()\n",
    "X = final_df.drop('label', axis=1)\n",
    "X = X.apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(final_df['label'])\n",
    "\n",
    "# Save label encoder for later decoding\n",
    "np.save(os.path.join(DATA_DIR, \"Time_classes.npy\"), label_encoder.classes_)\n",
    "print(\"ğŸ’¾ Saved label classes for later decoding\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 3ï¸âƒ£ Build TensorFlow Model ===\n",
    "print(\"\\nğŸ§  Building TensorFlow Model...\")\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# === 4ï¸âƒ£ Train Model ===\n",
    "print(\"\\nğŸš€ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 5ï¸âƒ£ Evaluate ===\n",
    "print(\"\\nğŸ“Š Evaluating model...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"ğŸ“‰ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# === 6ï¸âƒ£ Save Models (Keras, SavedModel, TFLite) ===\n",
    "TFMODELS_DIR = os.path.join(DATA_DIR, \"TimeModels\")\n",
    "os.makedirs(TFMODELS_DIR, exist_ok=True)\n",
    "\n",
    "KERAS_PATH = os.path.join(TFMODELS_DIR, \"ASL_Time_Model.keras\")\n",
    "SAVEDMODEL_PATH = os.path.join(TFMODELS_DIR, \"ASL_Time_SavedModel\")\n",
    "TFLITE_PATH = os.path.join(TFMODELS_DIR, \"ASL_Time_Model.tflite\")\n",
    "\n",
    "# Save .keras\n",
    "model.save(KERAS_PATH)\n",
    "print(f\"ğŸ’¾ Saved Keras model â†’ {KERAS_PATH}\")\n",
    "\n",
    "# Save as TensorFlow SavedModel\n",
    "model.export(SAVEDMODEL_PATH)\n",
    "print(f\"ğŸ’¾ Saved TensorFlow SavedModel â†’ {SAVEDMODEL_PATH}\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_PATH, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ğŸ’¾ Saved TFLite model â†’ {TFLITE_PATH}\")\n",
    "\n",
    "print(\"\\nâœ… All models exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5f75de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading TensorFlow model...\n",
      "âœ… Loaded model with 7 output labels\n",
      "ğŸ¥ Starting webcam... Press 'q' to quit.\n",
      "ğŸ– Show both hands clearly to the camera.\n",
      "ğŸ›‘ Webcam closed.\n"
     ]
    }
   ],
   "source": [
    "# Real-Time ASL Phrase Prediction (Two-Hand Version) using TensorFlow\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# === Paths ===\n",
    "MODEL_PATH = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\Tensorflow\\TFModels\\ObjectThingsModels\\ASL_ObjectThings_Model.keras\"\n",
    "LABEL_PATH = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\ASLBasicPhrasesSignLanguages\\ObjectThings\\ObjectThings_classes.npy\"\n",
    "\n",
    "# === Load model and labels ===\n",
    "print(\"ğŸ“¦ Loading TensorFlow model...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "label_classes = np.load(LABEL_PATH, allow_pickle=True)\n",
    "print(f\"âœ… Loaded model with {len(label_classes)} output labels\")\n",
    "\n",
    "# === MediaPipe Setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,                 # âœ… Allow both hands\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# === Helper: Extract both-hand keypoints (126 features = 2 Ã— 21 Ã— 3) ===\n",
    "def extract_two_hand_keypoints(results):\n",
    "    left_hand = np.zeros(21 * 3)\n",
    "    right_hand = np.zeros(21 * 3)\n",
    "\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            label = results.multi_handedness[hand_idx].classification[0].label\n",
    "            coords = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                coords.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "            if label.lower() == 'left':\n",
    "                left_hand = np.array(coords)\n",
    "            else:\n",
    "                right_hand = np.array(coords)\n",
    "\n",
    "    # Always return fixed-length 126-dim vector\n",
    "    return np.concatenate([left_hand, right_hand])\n",
    "\n",
    "# === Prediction Smoothing ===\n",
    "predictions_queue = deque(maxlen=10)\n",
    "\n",
    "# === Start Webcam ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"ğŸš« Cannot access webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"ğŸ¥ Starting webcam... Press 'q' to quit.\")\n",
    "print(\"ğŸ– Show both hands clearly to the camera.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"âš ï¸ Frame capture failed, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Flip and preprocess\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    # === Extract hand features (always 126 features) ===\n",
    "    features = extract_two_hand_keypoints(results).reshape(1, -1)\n",
    "\n",
    "    # === Predict if at least one hand is detected ===\n",
    "    if np.any(features):\n",
    "        probs = model.predict(features, verbose=0)\n",
    "        pred_idx = np.argmax(probs)\n",
    "        pred_label = label_classes[pred_idx]\n",
    "        confidence = probs[0][pred_idx]\n",
    "\n",
    "        predictions_queue.append(pred_label)\n",
    "        stable_prediction = max(set(predictions_queue), key=predictions_queue.count)\n",
    "\n",
    "        # Display text\n",
    "        cv2.putText(frame,\n",
    "                    f\"{stable_prediction} ({confidence*100:.1f}%)\",\n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                    (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"No Hands Detected\",\n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.0, (0, 0, 255), 2)\n",
    "\n",
    "    # === Draw landmarks for both hands ===\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0),\n",
    "                                       thickness=2, circle_radius=3),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0),\n",
    "                                       thickness=2)\n",
    "            )\n",
    "\n",
    "    # Show live frame\n",
    "    cv2.imshow(\"ASL Phrase Recognition (TensorFlow Two-Hand)\", frame)\n",
    "\n",
    "    # Exit condition\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# === Cleanup ===\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n",
    "print(\"ğŸ›‘ Webcam closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
