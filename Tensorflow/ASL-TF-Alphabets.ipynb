{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9f4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.3289 - loss: 2.4102 - val_accuracy: 0.6468 - val_loss: 1.4430\n",
      "Epoch 2/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7069 - loss: 1.0424 - val_accuracy: 0.7967 - val_loss: 0.7076\n",
      "Epoch 3/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8447 - loss: 0.5892 - val_accuracy: 0.8999 - val_loss: 0.4457\n",
      "Epoch 4/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8918 - loss: 0.4215 - val_accuracy: 0.8987 - val_loss: 0.3573\n",
      "Epoch 5/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9171 - loss: 0.3368 - val_accuracy: 0.9211 - val_loss: 0.2881\n",
      "Epoch 6/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9367 - loss: 0.2797 - val_accuracy: 0.9357 - val_loss: 0.2578\n",
      "Epoch 7/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9479 - loss: 0.2454 - val_accuracy: 0.9448 - val_loss: 0.2300\n",
      "Epoch 8/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9511 - loss: 0.2169 - val_accuracy: 0.9654 - val_loss: 0.1698\n",
      "Epoch 9/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.1963 - val_accuracy: 0.9691 - val_loss: 0.1570\n",
      "Epoch 10/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9649 - loss: 0.1766 - val_accuracy: 0.9666 - val_loss: 0.1460\n",
      "Epoch 11/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9627 - loss: 0.1728 - val_accuracy: 0.9539 - val_loss: 0.1775\n",
      "Epoch 12/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.1644 - val_accuracy: 0.9684 - val_loss: 0.1314\n",
      "Epoch 13/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9678 - loss: 0.1545 - val_accuracy: 0.9763 - val_loss: 0.1193\n",
      "Epoch 14/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9725 - loss: 0.1403 - val_accuracy: 0.9703 - val_loss: 0.1291\n",
      "Epoch 15/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9722 - loss: 0.1397 - val_accuracy: 0.9678 - val_loss: 0.1175\n",
      "Epoch 16/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9774 - loss: 0.1245 - val_accuracy: 0.9721 - val_loss: 0.1252\n",
      "Epoch 17/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1324 - val_accuracy: 0.9757 - val_loss: 0.1140\n",
      "Epoch 18/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9803 - loss: 0.1117 - val_accuracy: 0.9769 - val_loss: 0.1148\n",
      "Epoch 19/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9775 - loss: 0.1115 - val_accuracy: 0.9788 - val_loss: 0.1019\n",
      "Epoch 20/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9771 - loss: 0.1121 - val_accuracy: 0.9879 - val_loss: 0.0845\n",
      "Epoch 21/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0986 - val_accuracy: 0.9854 - val_loss: 0.0858\n",
      "Epoch 22/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9797 - loss: 0.0981 - val_accuracy: 0.9806 - val_loss: 0.0999\n",
      "Epoch 23/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9821 - loss: 0.0953 - val_accuracy: 0.9818 - val_loss: 0.0814\n",
      "Epoch 24/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9813 - loss: 0.0892 - val_accuracy: 0.9830 - val_loss: 0.0829\n",
      "Epoch 25/25\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9807 - loss: 0.0921 - val_accuracy: 0.9885 - val_loss: 0.0779\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9850 - loss: 0.1162\n",
      "âœ… Test Accuracy: 0.9850\n",
      "ğŸ’¾ Saved Keras model at: TFModels\\ASL_Alphabet_TF_Model.keras\n",
      "INFO:tensorflow:Assets written to: TFModels\\ASL_Alphabet_TF_Model_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TFModels\\ASL_Alphabet_TF_Model_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'TFModels\\ASL_Alphabet_TF_Model_SavedModel'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 63), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 26), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2930811275408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2930811276560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2930811274448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2930811274640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2930811277136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2930811274832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "ğŸ“‚ Exported TensorFlow SavedModel at: TFModels\\ASL_Alphabet_TF_Model_SavedModel\n",
      "ğŸ“¦ TensorFlow Lite model saved at: TFModels\\ASL_Alphabet_TF_Model.tflite\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# === Load dataset ===\n",
    "file_path = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\AlphabetSignLanguages\\ASL_\\Alphabetical_hand_sign_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === Prepare features and labels ===\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "y = pd.Categorical(df[\"label\"]).codes  # convert Aâ€“Z labels to numeric codes\n",
    "\n",
    "# === Split dataset ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Define Keras model ===\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(set(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# === Train model ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# === Evaluate model ===\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"âœ… Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# === Ensure save directory exists ===\n",
    "SAVE_DIR = \"TFModels\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# === 1ï¸âƒ£ Save in Keras format (for reloading in Python)\n",
    "keras_path = os.path.join(SAVE_DIR, \"ASL_Alphabet_TF_Model.keras\")\n",
    "model.save(keras_path)\n",
    "print(f\"ğŸ’¾ Saved Keras model at: {keras_path}\")\n",
    "\n",
    "# === 2ï¸âƒ£ Export as SavedModel (for TFLite conversion or mobile)\n",
    "export_dir = os.path.join(SAVE_DIR, \"ASL_Alphabet_TF_Model_SavedModel\")\n",
    "model.export(export_dir)\n",
    "print(f\"ğŸ“‚ Exported TensorFlow SavedModel at: {export_dir}\")\n",
    "\n",
    "# === 3ï¸âƒ£ Convert SavedModel â†’ TensorFlow Lite (.tflite)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = os.path.join(SAVE_DIR, \"ASL_Alphabet_TF_Model.tflite\")\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ğŸ“¦ TensorFlow Lite model saved at: {tflite_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2399677f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded Keras model.\n",
      "ğŸ¥ Starting real-time prediction... (Press ESC to exit)\n",
      "ğŸ›‘ Stopped real-time prediction.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# === Model paths ===\n",
    "KERAS_PATH = \"TFModels/ASL_Alphabet_TF_Model_SavedModel/ASL_Alphabet_TF_Model.keras\"\n",
    "SAVEDMODEL_PATH = \"TFModels/ASL_Alphabet_TF_Model_SavedModel\"\n",
    "TFLITE_PATH = \"TFModels/ASL_Alphabet_TF_Model_SavedModel/ASL_Alphabet_TF_Model.tflite\"\n",
    "\n",
    "# === Auto-detect model type ===\n",
    "USE_TFLITE = False\n",
    "USE_SAVEDMODEL = False\n",
    "\n",
    "if os.path.exists(KERAS_PATH):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(KERAS_PATH)\n",
    "        print(\"âœ… Loaded Keras model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to load .keras model: {e}\")\n",
    "        USE_SAVEDMODEL = True\n",
    "elif os.path.exists(SAVEDMODEL_PATH):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(SAVEDMODEL_PATH)\n",
    "        print(\"âœ… Loaded SavedModel.\")\n",
    "        USE_SAVEDMODEL = True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to load SavedModel: {e}\")\n",
    "        USE_TFLITE = True\n",
    "else:\n",
    "    print(\"âš ï¸ Loading TensorFlow Lite model...\")\n",
    "    USE_TFLITE = True\n",
    "\n",
    "# === Setup TensorFlow Lite interpreter if needed ===\n",
    "if USE_TFLITE:\n",
    "    interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print(\"âœ… Loaded TensorFlow Lite model.\")\n",
    "\n",
    "# === Mediapipe setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# === Labels (Aâ€“Z) ===\n",
    "labels = [chr(i) for i in range(65, 91)]\n",
    "\n",
    "# === Webcam setup ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "history = deque(maxlen=10)\n",
    "\n",
    "# Keep track of last valid prediction\n",
    "last_prediction = \"No hands detected\"\n",
    "no_hand_counter = 0\n",
    "NO_HAND_THRESHOLD = 15   # number of frames before showing \"No hands detected\"\n",
    "\n",
    "print(\"ğŸ¥ Starting real-time prediction... (Press ESC to exit)\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "    current_prediction = None\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        no_hand_counter = 0  # reset if hand is detected\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "\n",
    "            try:\n",
    "                if USE_TFLITE:\n",
    "                    interpreter.set_tensor(input_details[0]['index'], landmarks.astype(np.float32))\n",
    "                    interpreter.invoke()\n",
    "                    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "                else:\n",
    "                    output = model.predict(landmarks, verbose=0)\n",
    "\n",
    "                pred_idx = np.argmax(output)\n",
    "                current_prediction = labels[pred_idx] if pred_idx < len(labels) else \"?\"\n",
    "            except Exception as e:\n",
    "                current_prediction = f\"Error: {e}\"\n",
    "\n",
    "            history.append(current_prediction)\n",
    "    else:\n",
    "        # increment \"no hand\" counter\n",
    "        no_hand_counter += 1\n",
    "\n",
    "    # smooth prediction\n",
    "    if history:\n",
    "        smoothed_prediction = max(set(history), key=history.count)\n",
    "        if current_prediction:\n",
    "            last_prediction = smoothed_prediction\n",
    "        elif no_hand_counter > NO_HAND_THRESHOLD:\n",
    "            last_prediction = \"No hands detected\"\n",
    "\n",
    "    # display result\n",
    "    cv2.putText(frame, f\"Predicted: {last_prediction}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"ASL Alphabet Recognition (TensorFlow)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"ğŸ›‘ Stopped real-time prediction.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
