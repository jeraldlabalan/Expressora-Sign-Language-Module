{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc41965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === Configuration ===\n",
    "DATA_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\FSLFacialExpressionsLanguages\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Ask user for label name\n",
    "label = input(\"Effort\").strip().lower()\n",
    "SAVE_PATH = os.path.join(DATA_DIR, f\"{label}.csv\")\n",
    "\n",
    "# === Landmark indices (reduce redundancy) ===\n",
    "SELECTED_FACE_IDXS = [\n",
    "    33, 133, 362, 263,   # eyes\n",
    "    1, 2, 4, 5, 45, 275, # nose bridge + cheeks\n",
    "    61, 291, 0, 17, 57, 287  # mouth + chin\n",
    "]\n",
    "\n",
    "# === MediaPipe Setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face = mp.solutions.face_mesh\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.8,\n",
    "    min_tracking_confidence=0.8\n",
    ")\n",
    "face_mesh = mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.8\n",
    ")\n",
    "\n",
    "# === Smoothing helpers ===\n",
    "def smooth_landmarks(landmarks, prev_landmarks, alpha=0.5):\n",
    "    \"\"\"Apply exponential moving average to reduce jitter.\"\"\"\n",
    "    if prev_landmarks is None:\n",
    "        return landmarks\n",
    "    return alpha * np.array(landmarks) + (1 - alpha) * np.array(prev_landmarks)\n",
    "\n",
    "# === Dataset parameters ===\n",
    "MAX_SAMPLES = 600       # capture limit per label\n",
    "MIN_HAND_CONF = 0.75\n",
    "FRAME_SKIP = 2          # process every 2nd frame to reduce duplicates\n",
    "SMOOTHING = True\n",
    "\n",
    "# === Camera setup ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "data = []\n",
    "prev_landmarks = None\n",
    "\n",
    "print(f\"üì∑ Starting data capture for '{label}' in 3 seconds...\")\n",
    "time.sleep(3)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames to reduce redundancy\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb)\n",
    "    face_results = face_mesh.process(rgb)\n",
    "\n",
    "    row = []\n",
    "    hands_detected = False\n",
    "    face_detected = False\n",
    "\n",
    "    # === HAND LANDMARKS (80%) ===\n",
    "    hand_points = []\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks[:2]:\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                hand_points.extend([lm.x, lm.y, lm.z])\n",
    "        if len(hand_results.multi_hand_landmarks) == 1:\n",
    "            hand_points.extend([0] * (21 * 3))\n",
    "    else:\n",
    "        hand_points.extend([0] * (21 * 3 * 2))\n",
    "\n",
    "    # === FACE LANDMARKS (20%) ===\n",
    "    face_points = []\n",
    "    if face_results.multi_face_landmarks:\n",
    "        face_detected = True\n",
    "        face_landmarks = face_results.multi_face_landmarks[0]\n",
    "        for idx in SELECTED_FACE_IDXS:\n",
    "            lm = face_landmarks.landmark[idx]\n",
    "            face_points.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        face_points.extend([0] * (len(SELECTED_FACE_IDXS) * 3))\n",
    "\n",
    "    # === Combine all features ===\n",
    "    full_landmarks = hand_points + face_points\n",
    "\n",
    "    # Smooth landmarks to reduce noise\n",
    "    if SMOOTHING:\n",
    "        full_landmarks = smooth_landmarks(full_landmarks, prev_landmarks)\n",
    "    prev_landmarks = full_landmarks\n",
    "\n",
    "    # === Append metadata ===\n",
    "    row.extend(full_landmarks)\n",
    "    row.append(label)\n",
    "    row.append(datetime.utcnow().isoformat())\n",
    "\n",
    "    # Save only if valid hand detected\n",
    "    if hands_detected or face_detected:\n",
    "        data.append(row)\n",
    "        saved_count += 1\n",
    "        status = f\"‚úî Collected ({saved_count}/{MAX_SAMPLES})\"\n",
    "    else:\n",
    "        status = \"‚ùå No hands or face detected\"\n",
    "\n",
    "    # Stop once enough samples are collected\n",
    "    if saved_count >= MAX_SAMPLES:\n",
    "        print(\"\\n‚úÖ Capture complete.\")\n",
    "        break\n",
    "\n",
    "    # === Draw Landmarks ===\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for face_landmarks in face_results.multi_face_landmarks:\n",
    "            mp_draw.draw_landmarks(\n",
    "                frame, face_landmarks, mp_face.FACEMESH_TESSELATION,\n",
    "                mp_draw.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                mp_draw.DrawingSpec(color=(0, 0, 255), thickness=1)\n",
    "            )\n",
    "\n",
    "    # === HUD ===\n",
    "    cv2.putText(frame, f\"Label: {label}\", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.putText(frame, f\"Samples: {saved_count}/{MAX_SAMPLES}\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "    cv2.putText(frame, status, (10, 105),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if hands_detected else (0,0,255), 2)\n",
    "    cv2.imshow(\"FSL Capture (Improved v2)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"\\nüõë Manual stop triggered.\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === Save CSV ===\n",
    "columns = []\n",
    "for hand in [\"L1_\", \"L2_\"]:\n",
    "    for i in range(21):\n",
    "        columns += [f\"{hand}x{i}\", f\"{hand}y{i}\", f\"{hand}z{i}\"]\n",
    "for idx in SELECTED_FACE_IDXS:\n",
    "    columns += [f\"Fx{idx}\", f\"Fy{idx}\", f\"Fz{idx}\"]\n",
    "columns += [\"label\", \"timestamp\"]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "if not df.empty:\n",
    "    df.to_csv(SAVE_PATH, index=False)\n",
    "    print(f\"\\nüíæ Dataset saved: {SAVE_PATH}\")\n",
    "    print(f\"üìä Total samples: {len(df)} | Features per sample: {len(columns) - 2}\")\n",
    "else:\n",
    "    print(\"üö´ No valid samples collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21312da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration ===\n",
    "DATASET_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\FSLFacialExpressionsLanguages\\Expressions\"\n",
    "combined_path = os.path.join(DATASET_DIR, \"FSL_Facial_NMG_dataset.csv\")\n",
    "\n",
    "# === Scan for CSV files ===\n",
    "csv_files = [f for f in os.listdir(DATASET_DIR) if f.endswith(\".csv\")]\n",
    "print(f\"üìÅ Found {len(csv_files)} dataset files\")\n",
    "\n",
    "all_dfs = []\n",
    "all_columns = set()\n",
    "\n",
    "# === First pass: Collect all columns ===\n",
    "for csv_file in csv_files:\n",
    "    path = os.path.join(DATASET_DIR, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if df.empty:\n",
    "            print(f\"‚ö†Ô∏è Skipped empty file: {csv_file}\")\n",
    "            continue\n",
    "        if \"label\" not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è 'label' missing in {csv_file}, skipping...\")\n",
    "            continue\n",
    "        all_columns.update(df.columns)\n",
    "        print(f\"‚úÖ Columns collected from: {csv_file} ({len(df)} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {csv_file}: {e}\")\n",
    "\n",
    "if not all_columns:\n",
    "    raise SystemExit(\"üö´ No valid CSV files found.\")\n",
    "\n",
    "# === Enforce column order identical to capture script ===\n",
    "SELECTED_FACE_IDXS = [\n",
    "    33,133,362,263,   # eyes\n",
    "    1,2,4,5,45,275,   # nose + cheeks\n",
    "    61,291,0,17,57,287 # mouth + chin\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for hand in [\"L1_\", \"L2_\"]:\n",
    "    for i in range(21):\n",
    "        columns += [f\"{hand}x{i}\", f\"{hand}y{i}\", f\"{hand}z{i}\"]\n",
    "for idx in SELECTED_FACE_IDXS:\n",
    "    columns += [f\"Fx{idx}\", f\"Fy{idx}\", f\"Fz{idx}\"]\n",
    "columns += [\"label\", \"timestamp\"]\n",
    "\n",
    "# Use this explicit order instead of sorted(all_columns)\n",
    "all_columns = columns\n",
    "print(f\"‚úÖ Using fixed column order from capture script ({len(all_columns)} columns)\")\n",
    "\n",
    "# === Second pass: Align and merge ===\n",
    "for csv_file in csv_files:\n",
    "    path = os.path.join(DATASET_DIR, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if df.empty or \"label\" not in df.columns:\n",
    "            continue\n",
    "        df[\"label\"] = df[\"label\"].astype(str).str.strip().str.lower()\n",
    "        df = df.reindex(columns=all_columns, fill_value=0)\n",
    "        all_dfs.append(df)\n",
    "        print(f\"‚úÖ Loaded {csv_file} ({len(df)} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {csv_file}: {e}\")\n",
    "\n",
    "# === Merge all datasets ===\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    final_df.to_csv(combined_path, index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Combined dataset created successfully!\")\n",
    "    print(f\"üìÑ Saved to: {combined_path}\")\n",
    "    print(\"üßÆ Total samples:\", final_df.shape[0])\n",
    "\n",
    "    if \"label\" in final_df.columns:\n",
    "        print(\"\\nüìä Samples per label:\")\n",
    "        print(final_df[\"label\"].value_counts())\n",
    "        print(\"\\nüè∑Ô∏è Unique labels:\", list(final_df[\"label\"].unique()))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'label' column missing in merged file!\")\n",
    "else:\n",
    "    print(\"üö´ No valid datasets found to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc755840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import joblib\n",
    "\n",
    "# === Configuration ===\n",
    "DATA_PATH = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\FSLFacialExpressionsLanguages\\FSL_Facial_NMG_dataset.csv\"\n",
    "MODEL_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\TensorFlow\\FSL-FacialNonManualGrammarLanguages\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"nmg_model_tf.keras\")\n",
    "TFLITE_PATH = os.path.join(MODEL_DIR, \"nmg_model_tf.tflite\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# === Step 1: Load and Inspect Dataset ===\n",
    "print(\"üì• Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape[0]} samples, {df.shape[1]} columns\")\n",
    "\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\"üö´ 'label' column not found in dataset!\")\n",
    "\n",
    "# === Step 2: Clean Data ===\n",
    "X = df.drop(columns=[\"label\", \"timestamp\"], errors=\"ignore\").fillna(0)\n",
    "y = df[\"label\"]\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "# Replace zeros with NaN and fill with column means to stabilize scaling\n",
    "X = X.replace(0, np.nan)\n",
    "X = X.fillna(X.mean())\n",
    "print(f\"üîß Replaced zeros with feature means ‚Üí shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"üî¢ Feature matrix shape: {X.shape}\")\n",
    "print(\"üè∑Ô∏è Labels found:\", y.unique())\n",
    "\n",
    "# === Step 3: Balance Classes ===\n",
    "print(\"‚öñÔ∏è Balancing dataset...\")\n",
    "max_samples = y.value_counts().max()\n",
    "df_balanced = df.groupby(\"label\", group_keys=False).apply(\n",
    "    lambda x: resample(x, replace=True, n_samples=max_samples, random_state=42)\n",
    ")\n",
    "X = df_balanced.drop(columns=[\"label\", \"timestamp\"], errors=\"ignore\").fillna(0)\n",
    "y = df_balanced[\"label\"]\n",
    "print(df_balanced[\"label\"].value_counts())\n",
    "\n",
    "# === Step 4: Encode Labels ===\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"üî¢ Encoded {num_classes} classes ‚Üí\", list(label_encoder.classes_))\n",
    "\n",
    "# === Step 5: Normalize Features ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Step 6: Split Data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "print(\"üìä Data split complete (80% train / 20% test)\")\n",
    "\n",
    "# === Step 7: Define Neural Network Model ===\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),  # ‚úÖ compatible version\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# === Step 8: Train Model with EarlyStopping ===\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor='val_accuracy', verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,          # ‚¨ÖÔ∏è longer training\n",
    "    batch_size=16,       # ‚¨ÖÔ∏è smaller batch for finer updates\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === Step 9: Print Accuracy Summary per Epoch ===\n",
    "print(\"\\nüìà Training Accuracy Progress:\")\n",
    "for epoch, (train_acc, val_acc) in enumerate(zip(history.history['accuracy'], history.history['val_accuracy']), 1):\n",
    "    print(f\"Epoch {epoch:03d} ‚Üí Train: {train_acc:.4f} | Val: {val_acc:.4f}\")\n",
    "\n",
    "# === Step 10: Final Accuracy Summary ===\n",
    "train_acc_final = history.history['accuracy'][-1] * 100\n",
    "val_acc_final = history.history['val_accuracy'][-1] * 100\n",
    "print(f\"\\n‚úÖ Final Training Accuracy: {train_acc_final:.2f}%\")\n",
    "print(f\"‚úÖ Final Validation Accuracy: {val_acc_final:.2f}%\")\n",
    "\n",
    "\n",
    "# === Step 11: Evaluate Model ===\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüéØ Test Accuracy: {acc * 100:.2f}%\")\n",
    "print(\"\\nüìÑ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "print(\"üßæ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# === Step 12: Save Assets ===\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, \"nmg_model_tf_scaler.pkl\"))\n",
    "np.save(os.path.join(MODEL_DIR, \"nmg_model_tf_labels.npy\"), label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nüíæ Model saved ‚Üí {MODEL_PATH}\")\n",
    "print(f\"üìè Scaler saved ‚Üí {MODEL_DIR}\\\\nmg_model_tf_scaler.pkl\")\n",
    "print(f\"üè∑Ô∏è Labels saved ‚Üí {MODEL_DIR}\\\\nmg_model_tf_labels.npy\")\n",
    "\n",
    "# === Step 13: Convert to TFLite ===\n",
    "print(\"\\n‚öôÔ∏è Converting to TensorFlow Lite (.tflite)...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_PATH, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ TFLite model saved ‚Üí {TFLITE_PATH}\")\n",
    "print(\"\\nüèÅ Training + Conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque, Counter\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_DIR = r\"C:\\Users\\JamJayDatuin\\Documents\\Machine Learning Projects\\TrainingAI-Models\\Tensorflow\\ASL-NonManualGrammar-Recognition-TFModels\\NonManualGrammarModels\"\n",
    "TFLITE_MODEL_PATH = f\"{MODEL_DIR}\\\\nmg_model_tf.tflite\"\n",
    "SCALER_PATH       = f\"{MODEL_DIR}\\\\nmg_model_tf_scaler.pkl\"\n",
    "LABELS_PATH       = f\"{MODEL_DIR}\\\\nmg_model_tf_labels.npy\"\n",
    "\n",
    "# === Load Model and Preprocessors ===\n",
    "print(\"üì¶ Loading TFLite model and preprocessing assets...\")\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "labels = np.load(LABELS_PATH, allow_pickle=True)\n",
    "print(f\"‚úÖ Model loaded ‚Äî expects {input_details[0]['shape'][1]} features\")\n",
    "print(f\"üè∑Ô∏è Classes: {list(labels)}\")\n",
    "\n",
    "# === MediaPipe Setup ===\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face = mp.solutions.face_mesh\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.8,\n",
    "    min_tracking_confidence=0.8\n",
    ")\n",
    "face_mesh = mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.8\n",
    ")\n",
    "\n",
    "# === Selected Face Landmarks (same as training) ===\n",
    "SELECTED_FACE_IDXS = [\n",
    "    33,133,362,263,   # eyes\n",
    "    1,2,4,5,45,275,   # nose + cheeks\n",
    "    61,291,0,17,57,287 # mouth + chin\n",
    "]\n",
    "\n",
    "# === Prediction Stability Settings ===\n",
    "WINDOW_SIZE = 10\n",
    "CONF_THRESHOLD = 0.60         # baseline prediction confidence\n",
    "UNKNOWN_THRESHOLD = 0.35      # for \"Can't be Found\"\n",
    "prediction_window = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "def majority_vote(predictions):\n",
    "    \"\"\"Returns most common prediction within the window.\"\"\"\n",
    "    if not predictions:\n",
    "        return None\n",
    "    counter = Counter(predictions)\n",
    "    return counter.most_common(1)[0][0]\n",
    "\n",
    "# === Start Camera ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"üì∏ Camera started ‚Äî press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # === Detect hands and face ===\n",
    "    hand_results = hands.process(rgb)\n",
    "    face_results = face_mesh.process(rgb)\n",
    "\n",
    "    row = []\n",
    "    hands_detected = False\n",
    "    face_detected = False\n",
    "\n",
    "    # --- HAND LANDMARKS (2 hands) ---\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks[:2]:\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                row.extend([lm.x, lm.y, lm.z])\n",
    "        if len(hand_results.multi_hand_landmarks) == 1:\n",
    "            row.extend([0] * (21 * 3))\n",
    "    else:\n",
    "        row.extend([0] * (21 * 3 * 2))\n",
    "\n",
    "    # --- FACE LANDMARKS ---\n",
    "    if face_results.multi_face_landmarks:\n",
    "        face_detected = True\n",
    "        for idx in SELECTED_FACE_IDXS:\n",
    "            lm = face_results.multi_face_landmarks[0].landmark[idx]\n",
    "            row.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        row.extend([0] * (len(SELECTED_FACE_IDXS) * 3))\n",
    "\n",
    "    # === PREDICTION LOGIC ===\n",
    "    if not (hands_detected or face_detected):\n",
    "        text = \"No Hands or Face to Detect\"\n",
    "        color = (0, 0, 255)  # red\n",
    "    elif len(row) == input_details[0]['shape'][1]:\n",
    "        X_live = pd.DataFrame([row])\n",
    "        X_scaled = scaler.transform(X_live).astype(np.float32)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], X_scaled)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "        pred_conf = np.max(output)\n",
    "        pred_label = labels[np.argmax(output)]\n",
    "\n",
    "        # --- Unknown Gesture / Low Confidence ---\n",
    "        if pred_conf < UNKNOWN_THRESHOLD:\n",
    "            text = \"Can't be Found\"\n",
    "            color = (0, 165, 255)  # orange\n",
    "            prediction_window.clear()  # reset stability buffer\n",
    "        elif pred_conf >= CONF_THRESHOLD:\n",
    "            prediction_window.append(pred_label)\n",
    "            stable_pred = majority_vote(prediction_window)\n",
    "            text = f\"{stable_pred.upper()} ({pred_conf:.2f})\"\n",
    "            color = (0, 255, 0)  # green\n",
    "        else:\n",
    "            text = f\"Low Confidence ({pred_conf:.2f})\"\n",
    "            color = (0, 255, 255)  # yellow\n",
    "    else:\n",
    "        text = \"‚ö†Ô∏è Invalid landmark vector\"\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "    # === Draw Landmarks ===\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hl in hand_results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hl, mp_hands.HAND_CONNECTIONS)\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for fl in face_results.multi_face_landmarks:\n",
    "            mp_draw.draw_landmarks(\n",
    "                frame, fl, mp_face.FACEMESH_TESSELATION,\n",
    "                mp_draw.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                mp_draw.DrawingSpec(color=(0, 0, 255), thickness=1)\n",
    "            )\n",
    "\n",
    "    # === Overlay prediction ===\n",
    "    cv2.putText(frame, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "    cv2.putText(frame, f\"Frames stabilized: {len(prediction_window)}\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow(\"ASL Facial + Hand Expression (TFLite)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"üëã Session ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
